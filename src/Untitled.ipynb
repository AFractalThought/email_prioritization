{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90d281-1a97-4489-b7ba-1f007eb569c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output_var\n",
    "%pip install python-dotenv supabase\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from store import supabase_io\n",
    "from ml import pre_process, train, predict, evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a8bdf-ad19-43f5-b7e3-7af3e93544e4",
   "metadata": {},
   "source": [
    "## Train Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee5c253-3d73-4b53-8d6a-6bb6727b2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"emails_labeled\"\n",
    "\n",
    "# fetch table from Supabase\n",
    "df = supabase_io.fetch_df(table, limit=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b68ec088-5838-4984-83d1-689bf2d370b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['promotions', 'spam', 'social_media', 'forum', 'verify_code',\n",
       "       'updates'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350724a6-ce12-4a0b-831c-da7ee727300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train \n",
    "\n",
    "# Preprocess table (de-duplicate etc)\n",
    "X, y = pre_process.prepare_xy(df)\n",
    "\n",
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=37, stratify=y\n",
    "    )\n",
    "# Build pipeline and fit on training data\n",
    "pipe = train.build_pipeline().fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feed03d-8bee-42b0-a824-e135df79acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload Artifact\n",
    "# save model locally and upload to supabase\n",
    "path = train.save_model_local(pipe, \"artifacts/pipeline.joblib\")\n",
    "supabase_io.upload_artifact(path, bucket = \"models\", object_path = \"resend/v1/pipeline.joblib\", upsert = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79eea96-9b17-4aca-8c1e-d7aed80b2126",
   "metadata": {},
   "source": [
    "## App code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1fc1cd-9192-4af8-90ab-79f87a20df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output_var\n",
    "%pip install python-dotenv supabase\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from store import supabase_io\n",
    "from ml import predict, evaluate, pre_process\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cb5643-d1e6-4754-a211-b8d9372004e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download artifact if doesn't already exist\n",
    "lp = supabase_io.download_artifact(bucket = \"models\",\n",
    "                                   object_path = \"resend/v1/pipeline.joblib\",\n",
    "                                   local_path = \".cache/pipeline.joblib\",)\n",
    "model = joblib.load(lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c39663-7342-4122-b2ca-d92d47a1c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"emails_labeled\"\n",
    "\n",
    "# fetch table from Supabase\n",
    "df = supabase_io.fetch_df(table, limit=20000)\n",
    "\n",
    "# Preprocess table (de-duplicate etc)\n",
    "X, y = pre_process.prepare_xy(df)\n",
    "\n",
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=37, stratify=y\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504a0230-3d1a-4cc7-98e0-66e3839c466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predict and Evaluate\n",
    "### Use for model evaluation module\n",
    "# Get y predictions using argmax policy, as well as the probabilities for each label category\n",
    "y_hat, probabilities = predict.probabilities_and_labels(model, X_test)\n",
    "\n",
    "# Make confusion matrix for argmax policy and use PR auc for threshold independent evaluation\n",
    "classes = model.named_steps[\"clf\"].classes_\n",
    "confusion_mat_df, fig = evaluate.plot_confusion_matrix_argmax(y_test, y_hat, classes, normalize = None)\n",
    "pr_auc_score, fig = evaluate.plot_pr_auc_macro_from_proba(y_test, probabilities, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91acb72-e4fc-49bc-b386-d2e4788139d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulate labeling of an email event (ie single prediction)\n",
    "subject = \"50% off\"\n",
    "body = \"buy now for 50% off\"\n",
    "y_hat, probabilities = predict.predict_one(model, subject, body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16dbd00-5418-40f1-9f14-3cef96047b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = predict.predict_one_with_reasons(model, subject, body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38149c-de63-49c7-9758-ba4cbc8b6dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, prob_pred, prob_all_labels, reasons = predict.predict_one_with_reasons(model, subject, body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ada46d-3fc1-4ee3-831d-6dcdcd231d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f30e0-817c-4d41-8786-e70cacd47988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572ebd55-626c-453c-8ef8-c6f8c982eaf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5428be67-8976-4716-8a21-09de7b5219bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7892\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7892/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "\n",
    "import gradio as gr\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from store import supabase_io\n",
    "from ml import predict, evaluate, pre_process\n",
    "\n",
    "# Load env once at startup\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# CACHED LOADERS\n",
    "# ----------------------------\n",
    "@lru_cache(maxsize=1)\n",
    "def get_model():\n",
    "    lp = supabase_io.download_artifact(\n",
    "        bucket=\"models\",\n",
    "        object_path=\"resend/v1/pipeline.joblib\",\n",
    "        local_path=\".cache/pipeline.joblib\",\n",
    "        force=False,  # set True only if you overwrite the same remote object_path\n",
    "    )\n",
    "    return joblib.load(lp)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_raw_df(limit: int = 20000) -> pd.DataFrame:\n",
    "    return supabase_io.fetch_df(\"emails_labeled\", limit=limit)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_eval_split():\n",
    "    \"\"\"\n",
    "    Fixed split so your performance tab is stable / reproducible.\n",
    "    \"\"\"\n",
    "    df = get_raw_df()\n",
    "    X, y = pre_process.prepare_xy(df)  # your dedupe + fillna + type coercion\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.40, random_state=37, stratify=y\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# TAB 1: SIMULATE EMAIL EVENT\n",
    "# ----------------------------\n",
    "def ui_predict_one(subject: str, body: str):\n",
    "    model = get_model()\n",
    "\n",
    "    label, confidence, probs, reasons = predict.predict_one_with_reasons(\n",
    "        model, subject=subject, body=body, top_k=10\n",
    "    )\n",
    "\n",
    "    # probs can be pd.Series or dict; normalize to dict for gr.Label\n",
    "    if hasattr(probs, \"to_dict\"):\n",
    "        probs_dict = probs.to_dict()\n",
    "    else:\n",
    "        probs_dict = dict(probs)\n",
    "\n",
    "    # Reasons -> markdown bullets\n",
    "    reasons_md = \"\\n\".join([f\"- {r}\" for r in reasons]) if reasons else \"_No strong features found._\"\n",
    "\n",
    "    return (\n",
    "        str(label),\n",
    "        float(confidence),\n",
    "        probs_dict,       # for gr.Label (bars)\n",
    "        reasons_md,\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# TAB 2: PERFORMANCE\n",
    "# ----------------------------\n",
    "def ui_run_eval():\n",
    "    model = get_model()\n",
    "    _, X_test, _, y_test = get_eval_split()\n",
    "\n",
    "    y_hat, proba = predict.probabilities_and_labels(model, X_test)\n",
    "\n",
    "    classes = list(model.named_steps[\"clf\"].classes_)\n",
    "\n",
    "    # ensure proba is numpy array for PR code\n",
    "    if isinstance(proba, pd.DataFrame):\n",
    "        proba_np = proba[classes].to_numpy()\n",
    "    else:\n",
    "        proba_np = proba\n",
    "\n",
    "    cm_df, cm_fig = evaluate.plot_confusion_matrix_argmax(\n",
    "        y_test, y_hat, classes=classes, normalize=None\n",
    "    )\n",
    "    pr_auc, pr_fig = evaluate.plot_pr_auc_macro_from_proba(\n",
    "        y_test, proba_np, classes=classes\n",
    "    )\n",
    "\n",
    "    return cm_df, cm_fig, float(pr_auc), pr_fig\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# TAB 3: DATAFRAME DISPLAY\n",
    "# ----------------------------\n",
    "def ui_show_df(n_rows: int):\n",
    "    df = get_raw_df()\n",
    "    return df.head(int(n_rows))\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# APP\n",
    "# ----------------------------\n",
    "with gr.Blocks(title=\"Email Classifier Demo\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "# Email Classifier\n",
    "\n",
    "**Data source:** [jason23322/high-accuracy-email-classifier](https://huggingface.co/datasets/jason23322/high-accuracy-email-classifier)  \n",
    "**License:** Apache-2.0  \n",
    "\n",
    "For training details, evaluation methodology, and how predictions/reasons are computed, see the project **README**.\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Tabs():\n",
    "        # ---- Tab 1\n",
    "        with gr.Tab(\"Simulate email event\"):\n",
    "            subject = gr.Textbox(label=\"Subject\", lines=1, placeholder=\"e.g. Verify your email\")\n",
    "            body = gr.Textbox(label=\"Body\", lines=8, placeholder=\"Paste email body here...\")\n",
    "\n",
    "            btn = gr.Button(\"Predict\")\n",
    "\n",
    "            out_label = gr.Textbox(label=\"Prediction\")\n",
    "            out_conf = gr.Number(label=\"Confidence (max probability)\")\n",
    "            out_probs = gr.Label(label=\"Probabilities (all labels)\", num_top_classes=10)\n",
    "            out_reasons = gr.Markdown(label=\"Reasons (top features)\")\n",
    "\n",
    "            btn.click(\n",
    "                fn=ui_predict_one,\n",
    "                inputs=[subject, body],\n",
    "                outputs=[out_label, out_conf, out_probs, out_reasons],\n",
    "            )\n",
    "\n",
    "        # ---- Tab 2\n",
    "        with gr.Tab(\"Performance\"):\n",
    "            gr.Markdown(\"Uses a fixed train/test split of the labeled dataset.\")\n",
    "            run_eval = gr.Button(\"Run evaluation\")\n",
    "\n",
    "            cm_table = gr.Dataframe(label=\"Confusion matrix (counts)\")\n",
    "            cm_plot = gr.Plot(label=\"Confusion matrix plot\")\n",
    "            pr_score = gr.Number(label=\"Macro PR-AUC (Average Precision)\")\n",
    "            pr_plot = gr.Plot(label=\"Macro Precisionâ€“Recall curve\")\n",
    "\n",
    "            run_eval.click(\n",
    "                fn=ui_run_eval,\n",
    "                inputs=[],\n",
    "                outputs=[cm_table, cm_plot, pr_score, pr_plot],\n",
    "            )\n",
    "\n",
    "        # ---- Tab 3\n",
    "        with gr.Tab(\"Dataframe\"):\n",
    "            n_rows = gr.Slider(5, 500, value=50, step=5, label=\"Rows to display\")\n",
    "            show = gr.Button(\"Show rows\")\n",
    "            df_view = gr.Dataframe(label=\"emails_labeled (preview)\", wrap=True)\n",
    "\n",
    "            show.click(fn=ui_show_df, inputs=[n_rows], outputs=[df_view])\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5424549d-4961-406f-b4a6-8ad600923e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
